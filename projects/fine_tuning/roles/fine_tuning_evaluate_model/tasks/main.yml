---
- name: Create the src directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src/"
    state: directory
    mode: '0755'

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts/"
    state: directory
    mode: '0755'

- name: Prepare the config file template
  template:
    src: "{{ fine_tuning_job_config_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/base_config.yaml"
    mode: '0400'

- name: Convert the config to json
  shell:
    set -o pipefail;

    cat "{{ artifact_extra_logs_dir }}/src/base_config.yaml"
     | yq > "{{ artifact_extra_logs_dir }}/src/config.json"

- name: Prepare the config ConfigMap
  shell:
    set -o pipefail;

    oc create cm {{ fine_tuning_evaluate_model_name }}-config
       -n {{ fine_tuning_evaluate_model_namespace }}
       --from-file=config.json=<(cat "{{ artifact_extra_logs_dir }}/src/config.json")
       --dry-run=client
       -oyaml
       | oc apply -f-

- name: Prepare the entrypoint ConfigMap
  shell:
    set -o pipefail;

    oc create cm {{ fine_tuning_evaluate_model_name }}-entrypoint
       -n {{ fine_tuning_evaluate_model_namespace }}
       --from-file=$(find "{{ fine_tuning_job_entrypoint_dir }}" -maxdepth 1 -not -type d | tr '\n' ,)/dev/null
       --dry-run=client
       -oyaml
       | tee -a "{{ artifact_extra_logs_dir }}/src/configmap_entrypoint.yaml"
       | oc apply -f-

- name: Prepare the template file
  template:
    src: "{{ fine_tuning_job_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/fine_tuning_job.yaml"
    mode: '0400'

- name: Delete the fine-tuning job, if it exists
  command:
    oc delete -f "{{ artifact_extra_logs_dir }}/src/fine_tuning_job.yaml" --ignore-not-found

- name: Create the fine-tuning job
  command:
    oc create -f "{{ artifact_extra_logs_dir }}/src/fine_tuning_job.yaml"

- name: Wait for the job completion
  block:
  - name: Wait for the Pod to start running
    shell:
      set -o pipefail;
      oc get pods -ltraining.kubeflow.org/job-name={{ fine_tuning_evaluate_model_name }}
         -n {{ fine_tuning_evaluate_model_namespace }}
         --no-headers | awk '{print $3}'
    register: wait_pod_start
    retries: 20
    delay: 5
    until: wait_pod_start.stdout in ["Running", "Error", "Init:Error", "Completed", "NotReady", "CrashLoopBackOff"]

  - name: Fail if the Pod did not start successfully
    fail: msg="Pod in error state"
    when: wait_pod_start.stdout in ["Error", "Init:Error", "CrashLoopBackOff"]

  - name: Get the name of the eval pod
    shell:
      set -o pipefail;
      oc get pods -ltraining.kubeflow.org/job-name={{ fine_tuning_evaluate_model_name }}
        -n {{ fine_tuning_evaluate_model_namespace }} -oname
        | cut -d/ -f2
    register: eval_pod_name_cmd

  - name: Wait for the main container to finish running
    shell:
      set -o pipefail;
      oc get pod {{ eval_pod_name_cmd.stdout }}
         -n {{ fine_tuning_evaluate_model_namespace }}
         -ojson
        | jq ".status.containerStatuses[] | select(.state.terminated != null) | .name" -r
    register: wait_pod_start
    retries: 180
    delay: 30
    until: '"pytorch" in wait_pod_start.stdout'

  - name: Check on final status of Pod
    shell:
      set -o pipefail;
      oc get pods -ltraining.kubeflow.org/job-name={{ fine_tuning_evaluate_model_name }}
         -n {{ fine_tuning_evaluate_model_namespace }}
         --no-headers | awk '{print $3}'
    register: fail_on_pytorch_cmd

  - name: Fail if the Pod did not complete properly
    fail: msg="Pod in '{{ fail_on_pytorch_cmd.stdout }}' state"
    when: fail_on_pytorch_cmd.stdout not in ["Running", "NotReady"]

  always:
  - name: Capture the state of the InferenceService Pod resource
    shell:
      set -o pipefail;

      oc get pod
         -ltraining.kubeflow.org/job-name={{ fine_tuning_evaluate_model_name }}
         -ojson
         -n {{ fine_tuning_evaluate_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod.json;

      oc get pod
         -ltraining.kubeflow.org/job-name={{ fine_tuning_evaluate_model_name }}
         -oyaml
         -n {{ fine_tuning_evaluate_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod.yaml;

      oc get pod
         -ltraining.kubeflow.org/job-name={{ fine_tuning_evaluate_model_name }}
         -owide
         -n {{ fine_tuning_evaluate_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod.status;

      oc describe pod
         -ltraining.kubeflow.org/job-name={{ fine_tuning_evaluate_model_name }}
         -n {{ fine_tuning_evaluate_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod.desc

      oc logs $(
        oc get pod
           -ltraining.kubeflow.org/job-name={{ fine_tuning_evaluate_model_name }}
           -n {{ fine_tuning_evaluate_model_namespace }}
           -oname | head -1)
        -n {{ fine_tuning_evaluate_model_namespace }}
        > {{ artifact_extra_logs_dir }}/artifacts/pod.log
    ignore_errors: true

  - name: Capture the state of the InferenceService Pod resource
    shell:
      set -o pipefail;

      oc get pytorchjob/{{ fine_tuning_evaluate_model_name }}
         -oyaml
         -n {{ fine_tuning_evaluate_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pytorchjob.yaml;

      oc get pytorchjob/{{ fine_tuning_evaluate_model_name }}
         -owide
         -n {{ fine_tuning_evaluate_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pytorchjob.status;

      oc describe pytorchjob/{{ fine_tuning_evaluate_model_name }}
         -n {{ fine_tuning_evaluate_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pytorchjob.desc
    ignore_errors: true

  - name: Copy the result files
    shell:
      oc cp {{ fine_tuning_evaluate_model_namespace }}/{{ eval_pod_name_cmd.stdout }}:/mnt/output/fine-tuning {{ artifact_extra_logs_dir }}/artifacts/ -c export
    ignore_errors: true

  - name: Delete the Pod
    shell:
      oc delete pod {{ eval_pod_name_cmd.stdout }} -n {{ fine_tuning_evaluate_model_namespace }}
    ignore_errors: true
