---
training_data_path: "/mnt/output/dataset.json" # aka DATASET_DEST
model_name_or_path: "/mnt/storage/model/{{ fine_tuning_evaluate_model_model_name }}"
tokenizer_name_or_path: "/mnt/storage/model/{{ fine_tuning_evaluate_model_model_name }}"

response_template: "\n### Label:"

output_dir: "/mnt/output/fine-tuning"

accelerate_launch_args:
  num_processes: 1

num_train_epochs: 1
per_device_train_batch_size: {{ fine_tuning_evaluate_model_per_device_train_batch_size }}
per_device_eval_batch_size: {{ fine_tuning_evaluate_model_per_device_eval_batch_size }}
gradient_accumulation_steps: 4
evaluation_strategy: 'no'
save_strategy: epoch
learning_rate: 1.0e-05
weight_decay: 0
lr_scheduler_type: cosine

include_tokens_per_second: true
dataset_text_field: output
use_flash_attn: {{ fine_tuning_evaluate_model_use_flash_attn }}
